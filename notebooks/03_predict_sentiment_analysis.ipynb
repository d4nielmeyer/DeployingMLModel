{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.3"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}},"colab":{"name":"03_predict_sentiment_analysis.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KNOo_wLplBaO"},"source":["## Part 3: Deploying as a FaaS\n","\n","<a href=\"https://colab.research.google.com/github/peckjon/hosting-ml-as-microservice/blob/master/part3/predict_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ausO_-xclBaQ"},"source":["### Download corpuses\n","\n","Since we won't be doing any model-training in this step, we don't need the 'movie_reviews' corpus. However, we will still need to extract features from our input before each prediction, so we make sure 'punkt' and 'stopwords' are available for tokenization and stopword-removal. If you added any other corpuses in Part 2, consider whether they'll be needed in the prediction step."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"7jm7HalGlBaR","executionInfo":{"status":"ok","timestamp":1624698593565,"user_tz":-120,"elapsed":2059,"user":{"displayName":"Daniel Meyer","photoUrl":"","userId":"01875097517544988126"}},"outputId":"59dbced3-59dd-406c-f76b-5097a0ad8044"},"source":["from nltk import download\n","\n","download('punkt')\n","download('stopwords')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"wjaX966dlBaR"},"source":["### Define feature extractor and bag-of-words converter\n","\n","IMPORTANT: your predictions will only work properly if you use the same feature extractor that you trained your model with, so copy your updated `extract_features` method over from Part 2, replacing the method below. "]},{"cell_type":"code","metadata":{"id":"Tp9pC6IxlBaS","executionInfo":{"status":"ok","timestamp":1624698681855,"user_tz":-120,"elapsed":197,"user":{"displayName":"Daniel Meyer","photoUrl":"","userId":"01875097517544988126"}}},"source":["from nltk.corpus import stopwords\n","from string import punctuation\n","\n","stopwords_eng = stopwords.words('english')\n","\n","def extract_features(words):\n","    return [w for w in words if w not in stopwords_eng and w not in punctuation]\n","\n","def bag_of_words(words):\n","    bag = {}\n","    for w in words:\n","        bag[w] = bag.get(w,0)+1\n","    return bag"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"egRhDmgMlBaS"},"source":["### Import your pickled model file (non-Colab version)\n","\n","In Part 2, we saved the trained model as \"sa_classifier.pickle\". Now we'll unpickle that file to get it back into memory. Either copy that file into the same folder as this notebook (\"part3\"), or adjust the path below to \"../part2/sa_classifier.pickle\" so it reads the file from the folder where it was saved."]},{"cell_type":"code","metadata":{"id":"Mt8tx0PplBaS","executionInfo":{"status":"ok","timestamp":1624698740387,"user_tz":-120,"elapsed":200,"user":{"displayName":"Daniel Meyer","photoUrl":"","userId":"01875097517544988126"}}},"source":["import pickle\n","import sys\n","\n","if not 'google.colab' in sys.modules:\n","    model_file = open('sa_classifier.pickle', 'rb')\n","    model = pickle.load(model_file)\n","    model_file.close()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilg5aW5clBaT"},"source":["### Import your pickled model file (Colab version)\n","\n","If you're running this notebook on Colab, we need to retrieve the pickled model from [Google Drive](https://drive.google.com) before we can unpickle it. This code looks for \"sa_classifier.pickle\" in a folder called \"Colab Output\"; if you have moved the file elsewhere, change the path below."]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"gyRiyT-JlBaT","executionInfo":{"status":"ok","timestamp":1624698899868,"user_tz":-120,"elapsed":26384,"user":{"displayName":"Daniel Meyer","photoUrl":"","userId":"01875097517544988126"}},"outputId":"f7c0e8f6-e0cb-460f-8614-5477a47d8dc1"},"source":["import pickle\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    !ls '/content/gdrive/My Drive/Deploying MLModel/output'\n","    model_file = open('/content/gdrive/My Drive/Deploying MLModel/output/sa_classifier.pickle','rb')\n","    model = pickle.load(model_file)\n","    model_file.close()\n","    print('Model loaded from /content/gdrive/My Drive/Deploying MLModel/output')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","sa_classifier.pickle\n","Model loaded from /content/gdrive/My Drive/Deploying MLModel/output\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V0SIADEalBaT"},"source":["### Define a method for prediction\n","\n","In the prediction step, we'll be taking a single piece of text input and asking the model to classify it. Models need the input for the prediction step to have the same format as the data provided during training -- so we must tokenize the input, run the same `extract_features` method that we used during training, and convert it to a bag of words before sending it to the model's `classify` method.\n","\n","Note: if you have (from Part 2) changed your `extract_features` method to accept the full text instead of a tokenized list, then you can omit the tokenization step here."]},{"cell_type":"code","metadata":{"id":"AqBMRmNtlBaU","executionInfo":{"status":"ok","timestamp":1624699051920,"user_tz":-120,"elapsed":211,"user":{"displayName":"Daniel Meyer","photoUrl":"","userId":"01875097517544988126"}}},"source":["from nltk.tokenize import word_tokenize\n","\n","def get_sentiment(review):\n","    words = word_tokenize(review)\n","    words = extract_features(words)\n","    words = bag_of_words(words)\n","    return model.classify(words)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"id":"WddiFjW5lBaU"},"source":["### Run a prediction\n","\n","Test out your `get_sentiment` method on some sample inputs of your own devising: try altering the two reviews below and see how your model performs. It won't be 100% correct, and we're mostly just looking to see that it is able to run at all, but if it seems to *always* be wrong, that may indicate you've missed a critical step above (e.g. you haven't copied over all the changes to your feature extractor from Part 2, or you've loaded the wrong model file, or provided un-tokenized text when a list of words was expected)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKnRbIgTlBaV","executionInfo":{"status":"ok","timestamp":1624699055248,"user_tz":-120,"elapsed":201,"user":{"displayName":"Daniel Meyer","photoUrl":"","userId":"01875097517544988126"}},"outputId":"e392ac19-42b2-4da6-f164-515a3bd23e2e"},"source":["positive_review = 'This movie is amazing, with witty dialog and beautiful shots.'\n","print('positive_review: '+get_sentiment(positive_review))\n","\n","negative_review = 'I hated everything about this unimaginitive mess. Two thumbs down!'\n","print('negative_review: '+get_sentiment(negative_review))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["positive_review: pos\n","negative_review: neg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4TlkI0HrqPKK"},"source":["**Why a FaaS?**\n","\n","Having a working prediction function that runs on our own computer is good for running a few test predictions, but we want to be able to connect this to other services and pieces of code. For that, we need this function to be running on a server somewhere, accessible via a web API.\n","\n","We could choose to provision an entire physical server, but that requires significant DevOps work, is expensive to maintain, and doesn’t automatically scale as load increases. Instead, we’ll explore the world of cloud computing.\n","\n","Functions-as-a-Service (FaaS) are standalone functions which run on cloud infrastructure. They are sometimes referred to as “serverless functions” (there are still servers somewhere, we just don’t need to worry about where or how many). With FaaS, we can focus on just the code we want to run, without worrying about configuring any infrastructure. The FaaS provider takes care of the details of where it will run, and makes extra copies of the function if they are needed. Our service will autoscale if we start making thousands of calls to it, and if we are making no calls, we don’t get billed at all."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"AVbtRKsBlBaV"},"source":[""],"execution_count":null,"outputs":[]}]}